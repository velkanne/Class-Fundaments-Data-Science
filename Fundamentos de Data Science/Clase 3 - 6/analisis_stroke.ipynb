{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Predictivo de Derrames Cerebrales (Stroke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook, realizaremos un análisis completo del conjunto de datos de derrames cerebrales. El objetivo es preprocesar los datos, visualizarlos y construir un modelo de machine learning para predecir si un paciente podría sufrir un derrame cerebral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ttvga\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3847015651.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    df = pd.read_csv('C:\\Users\\ttvga\\OneDrive\\Escritorio\\vel\\Clase de Fund DS\\Clase 3 - 6')\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar los datos\n",
    "# La ruta del archivo es relativa a la ubicación de este notebook\n",
    "df = pd.read_csv('C:\\Users\\ttvga\\OneDrive\\Escritorio\\vel\\Clase de Fund DS\\Clase 3 - 6')\n",
    "\n",
    "# Mostrar las primeras filas del dataframe\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Mostrar información general del dataset\n",
    "print(\"Información general del dataset:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Manejo de Valores Faltantes\n",
    "Notamos que la columna `bmi` tiene valores faltantes ('N/A'). Los reemplazaremos con la media de la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 'N/A' con NaN de numpy para que sea tratable numéricamente\n",
    "df['bmi'].replace('N/A', np.nan, inplace=True)\n",
    "\n",
    "# Convertir la columna bmi a tipo numérico\n",
    "df['bmi'] = pd.to_numeric(df['bmi'])\n",
    "\n",
    "# Calcular la media de la columna bmi\n",
    "mean_bmi = df['bmi'].mean()\n",
    "\n",
    "# Rellenar los valores NaN con la media\n",
    "df['bmi'].fillna(mean_bmi, inplace=True)\n",
    "\n",
    "print(f\"Valores faltantes en 'bmi' rellenados con la media: {mean_bmi:.2f}\")\n",
    "print(\"Verificación de valores nulos después de la limpieza:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Manejo de Datos Categóricos\n",
    "Convertiremos las variables categóricas (como 'gender') en variables numéricas usando One-Hot Encoding para que el modelo pueda procesarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la fila con 'Other' en género, ya que es una sola y puede complicar el modelo sin aportar mucho\n",
    "df = df[df['gender'] != 'Other']\n",
    "\n",
    "# Aplicar One-Hot Encoding a las variables categóricas\n",
    "df_processed = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "print(\"Columnas después del One-Hot Encoding:\")\n",
    "print(df_processed.columns)\n",
    "\n",
    "print(\"Primeras 5 filas del dataset procesado:\")\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización de Datos (Gráficos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Gráfico 1: Distribución de la variable objetivo 'stroke'\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='stroke', data=df)\n",
    "plt.title('Distribución de Casos de Derrame Cerebral')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 2: Distribución de la edad\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['age'], kde=True, bins=30)\n",
    "plt.title('Distribución de la Edad')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico 3: Correlación entre variables numéricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_processed.corr(), annot=False, cmap='coolwarm')\n",
    "plt.title('Mapa de Calor de Correlaciones')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construcción del Modelo Predictivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Preparación de Datos para el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separar las características (X) y la variable objetivo (y)\n",
    "X = df_processed.drop('stroke', axis=1)\n",
    "y = df_processed['stroke']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Escalar las características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {X_train.shape[0]} filas\")\n",
    "print(f\"Datos de prueba: {X_test.shape[0]} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Entrenamiento y Evaluación del Modelo (Regresión Logística)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Precisión (Accuracy) del modelo: {accuracy:.4f}\")\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Conclusión del Modelo\n",
    "El reporte de clasificación nos muestra que, aunque la precisión general es alta (alrededor del 95%), esto se debe a que el modelo predice muy bien los casos de 'no derrame' (clase 0), que son la gran mayoría. Sin embargo, el rendimiento para predecir los casos de 'sí derrame' (clase 1) es muy bajo (bajo 'recall'). Esto es común en datasets desbalanceados y sugiere que se necesitarían técnicas más avanzadas (como el sobremuestreo de la clase minoritaria con SMOTE) para obtener un modelo más útil en la práctica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
